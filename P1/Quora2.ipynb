{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quora 2\n",
    "\n",
    "Trying to match vocab to word embeddings - not working currently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richard\\Anaconda3\\envs\\tf15\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (56370, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "print(\"Train shape : \",train.shape)\n",
    "print(\"Test shape : \",test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
       "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split to train and val\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2018)\n",
    "## some config values \n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 100 # max number of words in a question to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, verbose =  True):\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1175509/1175509 [00:03<00:00, 386605.68it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = train_df[\"question_text\"].progress_apply(lambda x: x.split()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['What', 'have', 'been', 'the', 'best', 'exhibits', 'at', 'the', 'Museo', 'del', 'Prado', 'in', 'Madrid?']),\n",
       "       list(['How', 'can', 'I', 'rotate', 'batch', 'image', 'files?']),\n",
       "       list(['Which', 'is', 'the', 'best', 'cable', 'operator', 'in', 'Thane', 'west', 'area?']),\n",
       "       ...,\n",
       "       list(['Do', 'we', 'need', 'a', 'prescription', 'for', 'cough', 'syrup', 'in', 'Egypt?']),\n",
       "       list(['What', 'are', 'the', 'best', 'and', 'worst', 'aspects', 'of', 'being', 'a', 'travel', 'agent?']),\n",
       "       list(['Who', 'was', 'a', 'person', 'you', 'met', 'who', 'gave', 'a', 'very', 'good', 'vibe/', 'good-spirit', 'that', 'you', 'remained', 'friends', 'with', 'through', 'life?'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1175509/1175509 [00:02<00:00, 442285.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'What': 375815, 'have': 74917, 'been': 12323, 'the': 588148, 'best': 55168}\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(sentences)\n",
    "print({k: vocab[k] for k in list(vocab)[:5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\richard\\Anaconda3\\envs\\tf15\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "news_path = 'D:\\\\ml_code\\\\embeddings\\\\GoogleNews-vectors-negative300\\\\GoogleNews-vectors-negative300.bin'\n",
    "embeddings_index = KeyedVectors.load_word2vec_format(news_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator \n",
    "\n",
    "def check_coverage(vocab,embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 508819/508819 [00:00<00:00, 613188.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 24.31% of vocab\n",
      "Found embeddings for  85.88% of all text\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1175509/1175509 [00:07<00:00, 148671.60it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1306122/1306122 [00:03<00:00, 391127.20it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: clean_text(x))\n",
    "sentences = train[\"question_text\"].apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_numbers(x):\n",
    "\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1175509/1175509 [00:09<00:00, 128021.58it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1306122/1306122 [00:03<00:00, 423144.85it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1306122/1306122 [00:03<00:00, 420642.90it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\n",
    "sentences = train[\"question_text\"].progress_apply(lambda x: x.split())\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "\n",
    "mispell_dict = {'colour':'color',\n",
    "                'centre':'center',\n",
    "                'didnt':'did not',\n",
    "                'doesnt':'does not',\n",
    "                'isnt':'is not',\n",
    "                'shouldnt':'should not',\n",
    "                'favourite':'favorite',\n",
    "                'travelling':'traveling',\n",
    "                'counselling':'counseling',\n",
    "                'theatre':'theater',\n",
    "                'cancelled':'canceled',\n",
    "                'labour':'labor',\n",
    "                'organisation':'organization',\n",
    "                'wwii':'world war 2',\n",
    "                'citicise':'criticize',\n",
    "                'instagram': 'social medium',\n",
    "                'whatsapp': 'social medium',\n",
    "                'snapchat': 'social medium'\n",
    "\n",
    "                }\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1175509/1175509 [00:04<00:00, 285850.14it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1306122/1306122 [00:03<00:00, 394374.02it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1306122/1306122 [00:03<00:00, 386737.03it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1306122/1306122 [00:03<00:00, 413118.53it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "sentences = train[\"question_text\"].progress_apply(lambda x: x.split())\n",
    "to_remove = ['a','to','of','and']\n",
    "sentences = [[word for word in sentence if not word in to_remove] for sentence in tqdm(sentences)]\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>651064</th>\n",
       "      <td>7f8590ef60e30b4344fd</td>\n",
       "      <td>What have been the best exhibits at the Museo ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294259</th>\n",
       "      <td>fda9538a2e0a5b2dfc3c</td>\n",
       "      <td>How can I rotate batch image files</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205706</th>\n",
       "      <td>ec528b4e3abc3347cd21</td>\n",
       "      <td>Which is the best cable operator in Thane west...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460090</th>\n",
       "      <td>5a1a41ea2086f2264eab</td>\n",
       "      <td>How do I expand factor and simplify in algebra</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277213</th>\n",
       "      <td>fa4f394af94b2b094e15</td>\n",
       "      <td>Do you judge people often</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          qid  \\\n",
       "651064   7f8590ef60e30b4344fd   \n",
       "1294259  fda9538a2e0a5b2dfc3c   \n",
       "1205706  ec528b4e3abc3347cd21   \n",
       "460090   5a1a41ea2086f2264eab   \n",
       "1277213  fa4f394af94b2b094e15   \n",
       "\n",
       "                                             question_text  target  \n",
       "651064   What have been the best exhibits at the Museo ...       0  \n",
       "1294259                 How can I rotate batch image files       0  \n",
       "1205706  Which is the best cable operator in Thane west...       0  \n",
       "460090      How do I expand factor and simplify in algebra       0  \n",
       "1277213                          Do you judge people often       0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "651064     What have been the best exhibits at the Museo ...\n",
       "1294259                   How can I rotate batch image files\n",
       "1205706    Which is the best cable operator in Thane west...\n",
       "460090        How do I expand factor and simplify in algebra\n",
       "1277213                            Do you judge people often\n",
       "700336     How do I get above ## in ICSE ##th in all subj...\n",
       "312154                          How do I use the grid method\n",
       "977640     Can you hang a poster diagonally if you have a...\n",
       "648670     Do many Bulgarians consider Macedonia part of ...\n",
       "168063     Did you have any superb business India on your...\n",
       "47318      How is the current condition of journalism or ...\n",
       "858766     How would you write a short story involving Fo...\n",
       "1238649    How can I delete my question in the Partner Pr...\n",
       "59860                                     What is ## of ####\n",
       "117013     Would Steve Jobs still be successful if he wer...\n",
       "930095     What are the best books for HTML Bootstrap Jquery\n",
       "1213698    Is it possible to actually win prizes from cla...\n",
       "1236564    How many calories does it take to burn one pou...\n",
       "790665     What should be the limits of religious freedom...\n",
       "770778     Is actuarial science good compared to a degree...\n",
       "245017                            How popular are you online\n",
       "341871         Why do certain foods cause diarrhea in babies\n",
       "233184     What happens if we fail in anyone of the end s...\n",
       "1117946    Is there any trusted website where I can get o...\n",
       "92203      What is the perception of recruitment agencies...\n",
       "679491     What is the criteria for internship at Tata power\n",
       "72014      What is it like working at tracxn technologies...\n",
       "293371     What are the packages given to students who st...\n",
       "742822                             What is a research report\n",
       "1149991    If you were to take a two week trip to Europe ...\n",
       "                                 ...                        \n",
       "940831     Is it possible to find an apartment in New Yor...\n",
       "386389               What do you think about the word RILDEN\n",
       "60856                                 Where is Covert Bailey\n",
       "1080995    What is teh dimensional formula of eléctrico f...\n",
       "657254     Why should we respect all religions instead of...\n",
       "803863     Which is the best least common place you have ...\n",
       "529170                    How does Quora pay for the answers\n",
       "558645     What was Lloyd Hustvedt s inspiration to becom...\n",
       "1182094    What are Robert Sternberg s three types of int...\n",
       "880106     Are Twitter Facebook and Flickr allowed in Hon...\n",
       "937854                  Whats the saddest truth about humans\n",
       "1256011    Im a Christian but my best friend is gay I don...\n",
       "1225781       How do you get Hepatitis B from another person\n",
       "729324                       What does a VGA cable look like\n",
       "355065     What is the particular chart of the difference...\n",
       "854650            Where is xylem and phloem found in a plant\n",
       "137891     How can I hide an app icon in the Samsung J 3 ...\n",
       "168455     How can I know properties and applications of ...\n",
       "685141     Can men with a borderline personality disorder...\n",
       "930360           What are some modern existentialist authors\n",
       "432072     How do I delete previous account while I have ...\n",
       "539599         What is the mistake in our educational system\n",
       "501733     I have a small bottled drinking water business...\n",
       "1262780    Which is the best men Fashion online shop in I...\n",
       "747275                   Who and how invented the cystoscopy\n",
       "28738      What should an engineer do who has gap after e...\n",
       "216871                            Why does China bully Tibet\n",
       "722422     What is it like to go to court for custody of ...\n",
       "1083451    Who is the most down to earth person you have ...\n",
       "161086     What kind of a person was Marcus Aurelius What...\n",
       "Name: question_text, Length: 100, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['question_text'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "del embeddings_index\n",
    "import gc; gc.collect()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fill up the missing values\n",
    "train_X = train_df[\"question_text\"].fillna(\"_na_\").values\n",
    "val_X = val_df[\"question_text\"].fillna(\"_na_\").values\n",
    "test_X = test_df[\"question_text\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = tokenizer.texts_to_sequences(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat for the val and test\n",
    "val_X = tokenizer.texts_to_sequences(val_X)\n",
    "test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "## Pad the sentences to the same length\n",
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "val_X = pad_sequences(val_X, maxlen=maxlen)\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "## Get the target values\n",
    "train_y = train_df['target'].values\n",
    "val_y = val_df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the entire GloVe word embedding file into memory as a dictionary of\n",
    "#word to embedding array\n",
    "path = 'D:\\\\ml_code\\\\embeddings\\\\glove.840B.300d\\\\'\n",
    "EMBEDDING_FILE = f'{path}glove.840B.300d.txt'\n",
    "\n",
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE,errors='ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a matrix of one embedding for each word in the training\n",
    "#dataset. We can do that by enumerating all unique words in the Tokenizer.word index and\n",
    "#locating the embedding weight vector from the loaded GloVe embedding. The result is a matrix\n",
    "#of weights only for words we will see during training.\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "\n",
    "#initialise the embedding matrix\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "\n",
    "#for each word in vocab get weights\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 300)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 30000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 30001     \n",
      "=================================================================\n",
      "Total params: 15,030,001\n",
      "Trainable params: 15,030,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#prepare a first simple model\n",
    "# define model\n",
    "model = Sequential()\n",
    "input_length = Input(shape=(maxlen,))\n",
    "e = Embedding(max_features, embed_size, weights=[embedding_matrix], input_length=maxlen)\n",
    "model.add(e)\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1175509 samples, validate on 130613 samples\n",
      "Epoch 1/1\n",
      " - 83s - loss: 0.9964 - acc: 0.9378 - val_loss: 1.0027 - val_acc: 0.9378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x206e7a34fd0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run for just one epoch\n",
    "model.fit(train_X, train_y, batch_size=512, epochs=1, validation_data=(val_X, val_y),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130613/130613 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 4us/step\n"
     ]
    }
   ],
   "source": [
    "#apply to validation set\n",
    "pred_glove_val_y = model.predict([val_X], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score at threshold 0.1 is 0.0\n",
      "F1 score at threshold 0.11 is 0.0\n",
      "F1 score at threshold 0.12 is 0.0\n",
      "F1 score at threshold 0.13 is 0.0\n",
      "F1 score at threshold 0.14 is 0.0\n",
      "F1 score at threshold 0.15 is 0.0\n",
      "F1 score at threshold 0.16 is 0.0\n",
      "F1 score at threshold 0.17 is 0.0\n",
      "F1 score at threshold 0.18 is 0.0\n",
      "F1 score at threshold 0.19 is 0.0\n",
      "F1 score at threshold 0.2 is 0.0\n",
      "F1 score at threshold 0.21 is 0.0\n",
      "F1 score at threshold 0.22 is 0.0\n",
      "F1 score at threshold 0.23 is 0.0\n",
      "F1 score at threshold 0.24 is 0.0\n",
      "F1 score at threshold 0.25 is 0.0\n",
      "F1 score at threshold 0.26 is 0.0\n",
      "F1 score at threshold 0.27 is 0.0\n",
      "F1 score at threshold 0.28 is 0.0\n",
      "F1 score at threshold 0.29 is 0.0\n",
      "F1 score at threshold 0.3 is 0.0\n",
      "F1 score at threshold 0.31 is 0.0\n",
      "F1 score at threshold 0.32 is 0.0\n",
      "F1 score at threshold 0.33 is 0.0\n",
      "F1 score at threshold 0.34 is 0.0\n",
      "F1 score at threshold 0.35 is 0.0\n",
      "F1 score at threshold 0.36 is 0.0\n",
      "F1 score at threshold 0.37 is 0.0\n",
      "F1 score at threshold 0.38 is 0.0\n",
      "F1 score at threshold 0.39 is 0.0\n",
      "F1 score at threshold 0.4 is 0.0\n",
      "F1 score at threshold 0.41 is 0.0\n",
      "F1 score at threshold 0.42 is 0.0\n",
      "F1 score at threshold 0.43 is 0.0\n",
      "F1 score at threshold 0.44 is 0.0\n",
      "F1 score at threshold 0.45 is 0.0\n",
      "F1 score at threshold 0.46 is 0.0\n",
      "F1 score at threshold 0.47 is 0.0\n",
      "F1 score at threshold 0.48 is 0.0\n",
      "F1 score at threshold 0.49 is 0.0\n",
      "F1 score at threshold 0.5 is 0.0\n"
     ]
    }
   ],
   "source": [
    "#look for a better threshold\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_glove_val_y>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do a clean up\n",
    "del word_index, embeddings_index, all_embs, embedding_matrix, model\n",
    "import gc; gc.collect()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
